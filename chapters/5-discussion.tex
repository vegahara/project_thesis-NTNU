\chapter{Discussion}

This chapter will discuss the results from the landmark detectors and the proposed quality indicator. First, the quality indicator and its properties and weaknesses are discussed. Second, the 1D landmark detector is found to not consistently detect landmarks, is difficult to tune, and is deemed unusable in a feature-based SLAM pipeline. Third, the 2D landmark detector is found to be able to close to consistently detect landmarks and found intuitive to tune. Last, a comparison between the two landmark detectors is performed. 

\section{Quality indicator for side scan sonar data}

The proposed quality indicator returns meaningful estimates of the dataset's quality and reports poor quality whenever the AUV turns, thus, we are able to point out the parts of the sonar image where anomalies are expected to occur. First, looking at the path in the top left part of \cref{fig:path_and_quality_ind}, it is shown that the quality indicator estimates poor quality whenever the AUV turns sharply. For the straightening up after a sharp turn, for example, at a traveled distance of $70 m$, no poor quality is reported. Even though it is a turn, it is not sharp enough to cause significant overlapping. Further, inspecting the path around a traveled distance of $250 m$, we can again see that the AUV is straightening up after a sharp turn. The straightening is slightly sharper, and a few yellow markers can be seen, indicating slight overlaps for a few swaths. 

Second, in the bottom part of \cref{fig:path_and_quality_ind}, where a part of the path is displayed with two and two consecutive effective ground ranges and "x"s marking the overlapping, it is shown how this overlapping affects the quality indicator. The closer the overlapping is to the AUV, the lower the estimated quality of the data. For no overlapping in the effective ground range, a quality indicator of $1.00$ is achieved. Looking at the swaths, we see how mapping these swaths to a sonar image will affect the image's appearance. For the part of the image that represents the outer turn, the spacing between each swath will be larger the further away from the AUV we get, effectively compressing objects and making them look smaller than their real size in the sonar image. For the inner turn, it is more complicated. Firstly, the part of the image from the AUV until the swaths overlap will appear enlarged. Second, the part of the swath ranging from the point of overlap and away from the AUV will be flipped. Combining several swaths, we see how intertangled the information becomes with only a selection of six swaths. It is therefore expected that the image of the inner turn can present many different anomalies and that it will be hard to infer the information that appears. 

Finally, looking at the right sonar image in \cref{fig:path_and_quality_ind}, the black lines loosely divide the sonar image into good and poor-quality parts. The first to notice in the parts of poor quality (i.e., where the quality indicator differs from green in the image), most of the landmarks appear to be, as expected, distorted in some way. At around a traveled distance of $60 m$, a banana-formed landmark appears in both the left and right swath. The left swath is the outer turn, and we expect the banana-formed object to be compressed. Making basic geometric interpretations, we know that straight lines on the seafloor in the along-track direction will appear bent in the sonar image if observed during a turn. This has likely happened to the banana-formed landmark in the left swath. The right part of the swath is the inner turn; therefore, it is hard to infer anything from the information in the sonar image.

Even though the quality indicator can point out where distortion and poor quality can appear due to swath overlapping, it will to a lesser degree, be able to point out if we will experience distortions due to weak turning, change in speed, or a change in roll angle. Looking at \cref{fig:path_and_quality_ind} again, we can see that after turns one and three, there is a section where the AUV is straightening up and performing a weaker turn to approach its path. Looking at the corresponding parts of the right-side sonar image, it is evident that the image appears torn and much of the contrast disappears, especially in the left swath. The quality indicator marks a few swaths as weak yellow, but it is not a clear indicator of the torn image. In addition, since speed is not directly incorporated and roll isn't incorporated in the quality indicator, it will not be able to detect distortions from changes in these.

To summarise, we note that the indicator returns meaningful estimates when the situation is thought "easy", since it only infers on the overlapping of consecutive swaths and does not directly take into account changes in speed and the roll angle. The proposed indicator is thus brittle with respect to this situation in the sense that it will provide non-reliable indications when anomalies in the data occur from changing speed or roll angle. Further work should focus on incorporating changes in the roll angle and speed in the quality indicator, making it more general and able to estimate poor quality more accurately. 

\section{1D landmark detector using peak detection} \label{sec:disc_1D_landmark_detector}

As shown in \cref{fig:1D_norm_result_test}, the 1D landmark detector can detect some landmarks consistently, but most are not consistently detected. The top five landmarks are not close to being consistently detected. Two of the four landmarks at the bottom are consistently detected, and the last two are part of a larger landmark that is only partially consistently detected. If this landmark detection method is to be used in a SLAM pipeline, the data association would likely struggle to provide suitable landmark matches. 

Comparing the results in \cref{fig:1D_tuning_results}, it is evident that there is not much difference in the performance between using unnormalized and normalized data, but using normalized data in combination with morphological operators gives a significant increase in performance. Firstly, the choice between unnormalized and normalized data is not obvious from the tuning results, as there is an insignificant difference in the performance. However, normalized data is preferred over unnormalized data because it has the same intensity properties over the whole swath. This makes it possible to expect the same detection performance over the whole swath, unlike unnormalized data with a great difference in dynamic range across track. Further, because of the significant increase in performance, utilizing morphological operators on the normalized data is also the preferred choice. However, to filter out the thin landmarks and increase the consistency of the detected landmarks, much of the geometrical details and information are filtered out, not providing much more information than the position of the landmarks and, to some extent, their size. Depending on the application, this may be sufficient. Still, for use in a SLAM pipeline, the landmark detector should be able to extract as much information from the landmarks as possible. If it turns out that the information isn't needed further down the pipeline, it can easily be omitted. 

The process of tuning the 1D landmark detector was complex due to the parameters being tightly coupled and unintuitive since there was no direct relation between the threshold parameter and the desired properties of the detector. The latter can be seen in \cref{fig:1D_raw_tuning_training}, where three different thresholds are shown, including the chosen threshold of $E = 17$. A threshold of $E = 18$ makes some landmarks more consistent but also introduces a new echo landmark that is not consistently detected around swath number $400$. On the other hand, with a threshold of $E = 16$, only one echo landmark is removed around swath number $1700$, but the others do not seem to get more consistent. Throughout the tuning process, it became evident that it was very hard to sort out the landmarks of the desired size, as both small and large landmarks were added or removed if the threshold was altered. It was also evident that there was a tight coupling between the smoothing parameter and the threshold. A change in the smoothing parameter implied that the range where the threshold parameter gave somewhat acceptable results was drastically changed, making the tuning more complex. 

To sum up, the 1D landmark method was not able to detect landmarks consistently and, at the same time, produce an acceptable amount of false positives; therefore, we deem it usable in a landmark-based SLAM pipeline. In addition, its tuning process is complex and unintuitive, and the best parameter set found filtered out a lot of the geometrical information about the landmarks. 

\section{2D landmark detector using expert rules}

As shown in \cref{fig:2D_result_single_test}, the 2D method can, to a larger extent, detect consistent or near-consistent landmarks in the data, but still, not all detected landmarks are consistent. From the top, the first landmark is inconsistently detected. In addition, landmark number four from the top appears to be close to consistent, but it is hard to point out what would be the ground truth for the landmark. The remaining landmarks are consistently detected. Further, the results show that there are a few false negatives. Several of the larger landmarks are unwanted due to being hard to detect consistently, but some landmarks should be picked out. This points in the direction that the tuning could have been more relaxed, ensuring that fewer landmarks are filtered out. However, despite some drawbacks, with no false positives and close to consistent detection of the detected landmarks, we were able to achieve our objectives for the tuning and accomplish acceptable performance. 

The tuning process is simple and mostly intuitive, and we can easily tune the different parameters to filter out the landmarks with the wanted geometrical properties. The landmark detector pipeline and tuning process are shown in \cref{fig:2D_tuning_intensity_thres} and \cref{fig:2d_tuning_paramaters_training}. Because of the sequential inner workings of the detector, the tuning can also be done sequentially, making it possible to tune one parameter at a time. First, the intensity is tuned to pick out all landmarks consistently, together with an acceptable amount of false positives. The effect of the different intensity thresholds on the consistency and amount of false positives are observed in \cref{fig:2D_tuning_intensity_thres}. Next, the different geometrical parameters can be tuned to filter out the landmarks with the desired geometrical properties. The height parameters and the fill rate threshold directly relate to the sonar image, thus filtering on the height and fill rate seen in the image, and are, therefore, intuitive and interpretable. The area filtering is a bit more complex, as the area is corrected for the effect of the grazing angle. It is, therefore, not perfectly intuitive how it will affect the landmarks because its effect varies with how far across track the landmarks are from the AUV. However, achieving the tuning objectives and hence an acceptable performance was uncomplicated. All in all, the tuning can be said to be simple and mostly intuitive. 

To conclude, achieving the tuning objectives with the 2D landmark detector was uncomplicated because the detector possesses properties that make it simple and intuitive to tune, but it has some weaknesses regarding consistent landmark detection. Further work should focus on adapting the 2D landmark detector to work on corrected sonar images in the cartesian space. This would mean that the parameters can be invariant to the sonar system used and only concern the physical sizes on the seafloor, not the sizes in the sonar image. 

\section{Comparison}

For comparison of the two landmark detector methods, their ability to achieve the tuning objectives and their tuning process is assessed. The main tuning objective was to have as few as possible false positives, the secondary to generate consistent landmarks, while the number of false negatives was only slightly considered during tuning. This choice was made under the assumption that the geometrical properties of the landmarks will change depending on the gracing angle and azimuth angle they are observed under. Hence, data association will have a problematic starting point, to begin with, and providing it with several false positives will increase the probability that a wrong data association will be made. However, this choice is not trivial, and further work should investigate how the rate of false positives and false negatives affects the data association.

Both landmark detection methods can do detection with an acceptable level of false positives but suffer, to a varying degree, from not being able to detect the landmarks consistently. Both methods have no false positives and are thus able to fulfill the main tuning objective of no false positives. The second objective, detecting all landmarks consistently, is, to a lesser degree, fulfilled. The 1D landmark detector has a high degree of inconsistent detection of landmarks and is not able to fulfill the objective. The 2D method, on the other hand, is able to detect landmarks close to consistently. Based on the degree of fulfillment of the tuning objectives, the 2D candidate is the most promising candidate for use as a landmark detector in a feature-based SLAM pipeline. 

Even though the 2D landmark detector has almost twice as many tuning parameters, it is less complex and more intuitive to tune than the 1D landmark detector. The 1D landmark detector is deemed unintuitive and complex to tune due to the tuning parameters being tightly connected and without direct relation to the wanted detection properties. The 2D landmark detector, on the other hand, has sequential inner workings and non-dependent tuning parameters, making it mostly intuitive and non-complex to tune. The tuning is also worth considering for practical applications and, again, has the 2D landmark detector potential for being used in a feature-based SLAM pipeline. 

To summarise, the 1D landmark detector is not able to fulfill all tuning objectives and is unintuitive to tune, whereas the 2D landmark detector is close to fulfilling the tuning objectives and is intuitive and non-complex to tune. Based on this, further work should focus on the 2D landmark detector, improving its detection capabilities and its tuning properties.  
